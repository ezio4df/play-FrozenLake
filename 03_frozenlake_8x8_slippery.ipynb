{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# FrozenLake [8x8 | no random map | slip]",
   "id": "c3581523111864cc"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-27T11:55:43.561556434Z",
     "start_time": "2025-12-27T11:55:42.031989488Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "import cv2\n",
    "\n",
    "plt.style.use(['dark_background', 'seaborn-v0_8'])\n",
    "\n",
    "# seed\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Pytorch device:\", device)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch device: cuda\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Model",
   "id": "e3a08abec0c12af3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T12:53:47.857755060Z",
     "start_time": "2025-12-27T12:53:47.791831631Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class FrozenLake8x8V0(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hidden_size=128):\n",
    "        super(FrozenLake8x8V0, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, action_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ],
   "id": "b3db636ec346b7e6",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Replay Buffer",
   "id": "553cd9c3321fdcda"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T12:53:48.536615459Z",
     "start_time": "2025-12-27T12:53:48.497982044Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = map(np.stack, zip(*batch))\n",
    "        return state, action, reward, next_state, done\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ],
   "id": "3a3fb2d2341b5559",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Agent",
   "id": "af22cb8477291d8a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T12:53:48.869082042Z",
     "start_time": "2025-12-27T12:53:48.852917027Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, modelClass, state_size, action_size, lr=1e-3, gamma=0.99,\n",
    "                 buffer_size=10000, batch_size=64, target_update=100):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.target_update = target_update\n",
    "\n",
    "        # Q-network and target network\n",
    "        self.q_net = modelClass(state_size, action_size).to(device)\n",
    "        self.target_net = modelClass(state_size, action_size).to(device)\n",
    "        self.optimizer = optim.Adam(self.q_net.parameters(), lr=lr)\n",
    "\n",
    "        # Sync target network\n",
    "        self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "\n",
    "        self.memory = ReplayBuffer(buffer_size)\n",
    "        self.step_count = 0\n",
    "\n",
    "    def act(self, state, epsilon=0.0):\n",
    "        if random.random() < epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.q_net(state)\n",
    "        return q_values.argmax().item()\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.push(state, action, reward, next_state, done)\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        # sample batch\n",
    "        states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)\n",
    "        states = torch.FloatTensor(states).to(device)\n",
    "        actions = torch.LongTensor(actions).to(device)\n",
    "        rewards = torch.FloatTensor(rewards).to(device)\n",
    "        next_states = torch.FloatTensor(next_states).to(device)\n",
    "        dones = torch.BoolTensor(dones).to(device)\n",
    "\n",
    "        # current q-values\n",
    "        current_q_values = self.q_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        # next q-values frm target network\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_net(next_states).max(1)[0]\n",
    "            target_q_values = rewards + (self.gamma * next_q_values * (~dones))\n",
    "\n",
    "        # compute loss\n",
    "        loss = nn.MSELoss()(current_q_values, target_q_values)\n",
    "\n",
    "        # Optimize\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # update target network periodically\n",
    "        self.step_count += 1\n",
    "        if self.step_count % self.target_update == 0:\n",
    "            self.target_net.load_state_dict(self.q_net.state_dict())\n"
   ],
   "id": "1cc206a96bfe34ed",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training Loop",
   "id": "7d28cdc04f3f7c6a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T12:53:49.230903259Z",
     "start_time": "2025-12-27T12:53:49.213083873Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_env(render_mode=None):\n",
    "    env = gym.make(\"FrozenLake8x8-v1\",\n",
    "                   is_slippery=True,\n",
    "                   render_mode=render_mode)\n",
    "    state_size = env.observation_space.n\n",
    "    action_size = env.action_space.n  # 4 (left, down, right, up)\n",
    "\n",
    "    return env, state_size, action_size\n",
    "\n",
    "\n",
    "def one_hot_state(s, size):\n",
    "    vec = np.zeros(size)\n",
    "    vec[s] = 1.0\n",
    "    return vec\n",
    "\n",
    "\n",
    "def train_dqn(agent, env, state_size, action_size, episodes=2000, max_steps=100):\n",
    "    global history_rewards, history_epsilons\n",
    "\n",
    "    scores = deque(maxlen=100)  # for moving average\n",
    "    epsilon_start = 1.000\n",
    "    epsilon_end = 0.010\n",
    "    epsilon_decay = 0.9995\n",
    "\n",
    "    epsilon = epsilon_start\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    for episode in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        state = one_hot_state(state, state_size)\n",
    "        total_reward = 0\n",
    "\n",
    "        for t in range(max_steps):\n",
    "            action = agent.act(state, epsilon)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # FrozenLake gives sparse reward: 1 only if goal reached\n",
    "            if not truncated and terminated:\n",
    "                if reward == 0:  #fell into a hole\n",
    "                    reward = -1\n",
    "                else:  #completed game\n",
    "                    reward = +1\n",
    "            else:\n",
    "                reward = reward - 0.02  # optional shaping to encourage speed\n",
    "\n",
    "            next_state = one_hot_state(next_state, state_size)\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            agent.replay()\n",
    "\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        scores.append(total_reward)\n",
    "        epsilon = max(epsilon_end, epsilon_decay * epsilon)\n",
    "\n",
    "        # Log every episode for smooth plot\n",
    "        avg_score = np.mean(scores)\n",
    "        history_rewards.append(avg_score)\n",
    "        history_epsilons.append(epsilon)\n",
    "\n",
    "        if episode % 100 == 0:\n",
    "            avg_score = np.mean(scores)\n",
    "            print(f\"Episode {episode}, Avg Reward (last 100): {avg_score:.3f}, Epsilon: {epsilon:.3f}\")\n",
    "\n",
    "        if episode % 1000 == 0 and episode > 0:\n",
    "            # render 1 episode, to check progress\n",
    "            render_env, state_size, action_size = create_env(render_mode=\"human\")\n",
    "            evaluate_agent(agent, render_env, episodes=1, no_log=True)\n",
    "            render_env.close()\n",
    "\n",
    "    return agent, env\n",
    "\n",
    "\n",
    "def evaluate_agent(agent, env, episodes=10, max_steps=100, no_log=False):\n",
    "    state_size = env.observation_space.n\n",
    "\n",
    "    success = 0\n",
    "    for _ in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        state = one_hot_state(state, state_size)\n",
    "        for _ in range(max_steps):\n",
    "            action = agent.act(state, epsilon=0.0)  # greedy\n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            state = one_hot_state(state, state_size)\n",
    "            if terminated or truncated:\n",
    "                if reward == 1.0:\n",
    "                    success += 1\n",
    "                break\n",
    "\n",
    "    if not no_log:\n",
    "        print(f\"\\nSuccess rate over {episodes} episodes: {success}/{episodes} ({100 * success / episodes:.1f}%)\")\n"
   ],
   "id": "48c70a69464b7c82",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T12:53:49.531181261Z",
     "start_time": "2025-12-27T12:53:49.482113164Z"
    }
   },
   "cell_type": "code",
   "source": [
    "env, state_size, action_size = create_env()\n",
    "agent = DQNAgent(modelClass=FrozenLake8x8V0,\n",
    "                 state_size=state_size,\n",
    "                 action_size=action_size,\n",
    "                 lr=5e-5,\n",
    "                 gamma=0.99,\n",
    "                 buffer_size=50_000,\n",
    "                 batch_size=128,\n",
    "                 target_update=20)\n",
    "\n",
    "history_rewards, history_epsilons = ([], [])"
   ],
   "id": "868719206c6066eb",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-12-27T12:53:49.704803311Z"
    }
   },
   "cell_type": "code",
   "source": [
    "try:\n",
    "    agent, env = train_dqn(\n",
    "        agent=agent,\n",
    "        env=env,\n",
    "        state_size=state_size,\n",
    "        action_size=action_size,\n",
    "        episodes=10_000,\n",
    "        max_steps=500)\n",
    "finally:\n",
    "    # Plotting\n",
    "    print(f\"Model: {FrozenLake8x8V0.__name__}\", agent.q_net)\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "    color = 'tab:blue'\n",
    "    ax1.set_xlabel('Episode')\n",
    "    ax1.set_ylabel('Avg Reward (last 100)', color=color)\n",
    "    ax1.plot(history_rewards, color=color, label='Avg Reward')\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    if len(history_rewards) > 0:\n",
    "        y_min = np.floor(min(history_rewards) * 10) / 10\n",
    "        y_max = np.ceil(max(history_rewards) * 10) / 10\n",
    "        ax1.set_yticks(np.arange(y_min, y_max + 0.05, 0.1))\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    color = 'tab:red'\n",
    "    ax2.set_ylabel('Epsilon', color=color)\n",
    "    ax2.plot(history_epsilons, color=color, label='Epsilon', linestyle='--')\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    plt.title('Training Progress')\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "b247899a320c3143",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Episode 0, Avg Reward (last 100): -1.300, Epsilon: 1.000\n",
      "Episode 100, Avg Reward (last 100): -1.646, Epsilon: 0.951\n",
      "Episode 200, Avg Reward (last 100): -1.540, Epsilon: 0.904\n",
      "Episode 300, Avg Reward (last 100): -1.513, Epsilon: 0.860\n",
      "Episode 400, Avg Reward (last 100): -1.617, Epsilon: 0.818\n",
      "Episode 500, Avg Reward (last 100): -1.544, Epsilon: 0.778\n",
      "Episode 600, Avg Reward (last 100): -1.530, Epsilon: 0.740\n",
      "Episode 700, Avg Reward (last 100): -1.471, Epsilon: 0.704\n",
      "Episode 800, Avg Reward (last 100): -1.444, Epsilon: 0.670\n",
      "Episode 900, Avg Reward (last 100): -1.464, Epsilon: 0.637\n",
      "Episode 1000, Avg Reward (last 100): -1.449, Epsilon: 0.606\n",
      "Episode 1100, Avg Reward (last 100): -1.622, Epsilon: 0.577\n",
      "Episode 1200, Avg Reward (last 100): -1.874, Epsilon: 0.548\n",
      "Episode 1300, Avg Reward (last 100): -1.817, Epsilon: 0.522\n",
      "Episode 1400, Avg Reward (last 100): -1.749, Epsilon: 0.496\n",
      "Episode 1500, Avg Reward (last 100): -1.805, Epsilon: 0.472\n",
      "Episode 1600, Avg Reward (last 100): -1.926, Epsilon: 0.449\n",
      "Episode 1700, Avg Reward (last 100): -1.873, Epsilon: 0.427\n",
      "Episode 1800, Avg Reward (last 100): -1.817, Epsilon: 0.406\n",
      "Episode 1900, Avg Reward (last 100): -1.823, Epsilon: 0.386\n",
      "Episode 2000, Avg Reward (last 100): -1.894, Epsilon: 0.368\n",
      "Episode 2100, Avg Reward (last 100): -1.963, Epsilon: 0.350\n",
      "Episode 2200, Avg Reward (last 100): -1.669, Epsilon: 0.333\n",
      "Episode 2300, Avg Reward (last 100): -1.746, Epsilon: 0.316\n",
      "Episode 2400, Avg Reward (last 100): -1.847, Epsilon: 0.301\n",
      "Episode 2500, Avg Reward (last 100): -1.836, Epsilon: 0.286\n",
      "Episode 2600, Avg Reward (last 100): -1.774, Epsilon: 0.272\n",
      "Episode 2700, Avg Reward (last 100): -1.569, Epsilon: 0.259\n",
      "Episode 2800, Avg Reward (last 100): -1.744, Epsilon: 0.246\n",
      "Episode 2900, Avg Reward (last 100): -1.645, Epsilon: 0.234\n",
      "Episode 3000, Avg Reward (last 100): -1.738, Epsilon: 0.223\n",
      "Episode 3100, Avg Reward (last 100): -1.730, Epsilon: 0.212\n",
      "Episode 3200, Avg Reward (last 100): -1.687, Epsilon: 0.202\n",
      "Episode 3300, Avg Reward (last 100): -1.544, Epsilon: 0.192\n",
      "Episode 3400, Avg Reward (last 100): -1.531, Epsilon: 0.183\n",
      "Episode 3500, Avg Reward (last 100): -1.551, Epsilon: 0.174\n",
      "Episode 3600, Avg Reward (last 100): -1.616, Epsilon: 0.165\n",
      "Episode 3700, Avg Reward (last 100): -1.425, Epsilon: 0.157\n",
      "Episode 3800, Avg Reward (last 100): -1.503, Epsilon: 0.149\n",
      "Episode 3900, Avg Reward (last 100): -1.617, Epsilon: 0.142\n",
      "Episode 4000, Avg Reward (last 100): -1.564, Epsilon: 0.135\n",
      "Episode 4100, Avg Reward (last 100): -1.480, Epsilon: 0.129\n",
      "Episode 4200, Avg Reward (last 100): -1.486, Epsilon: 0.122\n",
      "Episode 4300, Avg Reward (last 100): -1.231, Epsilon: 0.116\n",
      "Episode 4400, Avg Reward (last 100): -1.179, Epsilon: 0.111\n",
      "Episode 4500, Avg Reward (last 100): -1.252, Epsilon: 0.105\n",
      "Episode 4600, Avg Reward (last 100): -1.464, Epsilon: 0.100\n",
      "Episode 4700, Avg Reward (last 100): -1.416, Epsilon: 0.095\n",
      "Episode 4800, Avg Reward (last 100): -1.019, Epsilon: 0.091\n",
      "Episode 4900, Avg Reward (last 100): -1.356, Epsilon: 0.086\n",
      "Episode 5000, Avg Reward (last 100): -1.245, Epsilon: 0.082\n",
      "Episode 5100, Avg Reward (last 100): -1.334, Epsilon: 0.078\n",
      "Episode 5200, Avg Reward (last 100): -1.523, Epsilon: 0.074\n",
      "Episode 5300, Avg Reward (last 100): -1.369, Epsilon: 0.071\n",
      "Episode 5400, Avg Reward (last 100): -1.204, Epsilon: 0.067\n",
      "Episode 5500, Avg Reward (last 100): -1.392, Epsilon: 0.064\n",
      "Episode 5600, Avg Reward (last 100): -1.202, Epsilon: 0.061\n",
      "Episode 5700, Avg Reward (last 100): -1.375, Epsilon: 0.058\n",
      "Episode 5800, Avg Reward (last 100): -1.230, Epsilon: 0.055\n",
      "Episode 5900, Avg Reward (last 100): -1.294, Epsilon: 0.052\n",
      "Episode 6000, Avg Reward (last 100): -1.103, Epsilon: 0.050\n",
      "Episode 6100, Avg Reward (last 100): -1.292, Epsilon: 0.047\n",
      "Episode 6200, Avg Reward (last 100): -1.238, Epsilon: 0.045\n",
      "Episode 6300, Avg Reward (last 100): -1.193, Epsilon: 0.043\n",
      "Episode 6400, Avg Reward (last 100): -1.009, Epsilon: 0.041\n",
      "Episode 6500, Avg Reward (last 100): -1.102, Epsilon: 0.039\n",
      "Episode 6600, Avg Reward (last 100): -1.118, Epsilon: 0.037\n",
      "Episode 6700, Avg Reward (last 100): -1.031, Epsilon: 0.035\n",
      "Episode 6800, Avg Reward (last 100): -1.023, Epsilon: 0.033\n",
      "Episode 6900, Avg Reward (last 100): -1.176, Epsilon: 0.032\n",
      "Episode 7000, Avg Reward (last 100): -1.115, Epsilon: 0.030\n",
      "Episode 7100, Avg Reward (last 100): -1.128, Epsilon: 0.029\n",
      "Episode 7200, Avg Reward (last 100): -1.070, Epsilon: 0.027\n",
      "Episode 7300, Avg Reward (last 100): -1.181, Epsilon: 0.026\n",
      "Episode 7400, Avg Reward (last 100): -1.055, Epsilon: 0.025\n",
      "Episode 7500, Avg Reward (last 100): -0.999, Epsilon: 0.023\n",
      "Episode 7600, Avg Reward (last 100): -0.911, Epsilon: 0.022\n",
      "Episode 7700, Avg Reward (last 100): -0.960, Epsilon: 0.021\n",
      "Episode 7800, Avg Reward (last 100): -0.992, Epsilon: 0.020\n",
      "Episode 7900, Avg Reward (last 100): -1.065, Epsilon: 0.019\n",
      "Episode 8000, Avg Reward (last 100): -1.026, Epsilon: 0.018\n",
      "Episode 8100, Avg Reward (last 100): -1.086, Epsilon: 0.017\n",
      "Episode 8200, Avg Reward (last 100): -1.054, Epsilon: 0.017\n",
      "Episode 8300, Avg Reward (last 100): -0.982, Epsilon: 0.016\n",
      "Episode 8400, Avg Reward (last 100): -1.065, Epsilon: 0.015\n",
      "Episode 8500, Avg Reward (last 100): -1.024, Epsilon: 0.014\n",
      "Episode 8600, Avg Reward (last 100): -1.126, Epsilon: 0.014\n",
      "Episode 8700, Avg Reward (last 100): -0.936, Epsilon: 0.013\n",
      "Episode 8800, Avg Reward (last 100): -0.919, Epsilon: 0.012\n",
      "Episode 8900, Avg Reward (last 100): -0.978, Epsilon: 0.012\n",
      "Episode 9000, Avg Reward (last 100): -1.126, Epsilon: 0.011\n",
      "Episode 9100, Avg Reward (last 100): -0.905, Epsilon: 0.011\n",
      "Episode 9200, Avg Reward (last 100): -1.213, Epsilon: 0.010\n",
      "Episode 9300, Avg Reward (last 100): -1.029, Epsilon: 0.010\n",
      "Episode 9400, Avg Reward (last 100): -1.027, Epsilon: 0.010\n",
      "Episode 9500, Avg Reward (last 100): -1.048, Epsilon: 0.010\n",
      "Episode 9600, Avg Reward (last 100): -0.909, Epsilon: 0.010\n",
      "Episode 9700, Avg Reward (last 100): -0.990, Epsilon: 0.010\n",
      "Episode 9800, Avg Reward (last 100): -1.189, Epsilon: 0.010\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# env.close()",
   "id": "228841fa39a2b2ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T11:18:22.464610979Z",
     "start_time": "2025-12-27T11:17:56.359665619Z"
    }
   },
   "cell_type": "code",
   "source": [
    "eval_env, state_size, action_size = create_env(render_mode=\"human\")\n",
    "evaluate_agent(agent, eval_env, episodes=1)\n",
    "eval_env.close()"
   ],
   "id": "2b0a49ff245095bd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Success rate over 1 episodes: 0/1 (0.0%)\n"
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T10:38:47.704606025Z",
     "start_time": "2025-12-27T10:38:46.362672771Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # Record an env\n",
    "# eval_env, state_size, action_size = create_env(render_mode=\"rgb_array\")\n",
    "# eval_env = RecordVideo(\n",
    "#     eval_env,\n",
    "#     video_folder=\"files\",\n",
    "#     name_prefix=\"frozenlake_8x8_slippery\",# # Record an env\n",
    "#     episode_trigger=lambda x: True\n",
    "# )\n",
    "# evaluate_agent(agent, eval_env, episodes=1, max_steps=1000)\n",
    "# eval_env.close()\n",
    "#\n",
    "# # ffmpeg -i input.mp4 -vf \"fps=10,scale=320:-1:flags=lanczos\" -loop 0 output.gif"
   ],
   "id": "385845ea2291dc6c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Success rate over 1 episodes: 1/1 (100.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function RecordVideo.__del__ at 0x7f702f3a60c0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ezio4df/projects/play-FrozenLake/.venv/lib/python3.13/site-packages/gymnasium/wrappers/rendering.py\", line 433, in __del__\n",
      "    if len(self.recorded_frames) > 0:\n",
      "AttributeError: 'RecordVideo' object has no attribute 'recorded_frames'\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-27T10:42:34.289882091Z",
     "start_time": "2025-12-27T10:42:34.238864591Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MODEL_PATH = \"files/dqn_frozenlake8x8_slippery.pth\"\n",
    "\n",
    "# save model\n",
    "torch.save(agent.q_net.state_dict(), MODEL_PATH)\n",
    "\n",
    "# load model\n",
    "# agent.q_net.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "# agent.target_net.load_state_dict(agent.q_net.state_dict())  # Sync target net!"
   ],
   "id": "bc0ad62dc88bf73b",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5deac4996a271f8d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
