{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# FrozenLake [8x8 | random map | no slip]",
   "id": "5899a6d6f4260d55"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-30T05:02:41.879686882Z",
     "start_time": "2025-12-30T05:02:41.813543878Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "from gymnasium.envs.toy_text.frozen_lake import generate_random_map\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "plt.style.use(['dark_background', 'seaborn-v0_8'])\n",
    "\n",
    "# seed\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Pytorch device:\", device)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch device: cuda\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T05:02:42.034291433Z",
     "start_time": "2025-12-30T05:02:42.005256011Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class FrozenLakeImageWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env, img_size=(84, 84), grayscale=False):\n",
    "        super().__init__(env)\n",
    "        self.img_size = img_size\n",
    "        self.grayscale = grayscale\n",
    "        channels = 1 if grayscale else 3\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0, high=255,\n",
    "            shape=(*img_size, channels),\n",
    "            dtype=np.uint8\n",
    "        )\n",
    "\n",
    "    def observation(self, _):\n",
    "        frame = self.env.render()\n",
    "        if frame is None:\n",
    "            channels = 1 if self.grayscale else 3\n",
    "            return np.zeros((*self.img_size, channels), dtype=np.uint8)\n",
    "\n",
    "        # Convert to PIL Image\n",
    "        img = Image.fromarray(frame)\n",
    "\n",
    "        # Grayscale conversion\n",
    "        if self.grayscale:\n",
    "            img = img.convert('L')\n",
    "\n",
    "        # Resize\n",
    "        img = img.resize(self.img_size, Image.NEAREST)\n",
    "        obs = np.array(img, dtype=np.uint8)\n",
    "\n",
    "        # Add channel dimension for grayscale\n",
    "        if self.grayscale:\n",
    "            obs = np.expand_dims(obs, axis=-1)\n",
    "\n",
    "        return obs\n",
    "\n",
    "\n",
    "def preprocess_frame(frame, resize=(84, 84), grayscale=True):\n",
    "    \"\"\"For testing preprocessing\"\"\"\n",
    "    img = Image.fromarray(frame)\n",
    "    if grayscale:\n",
    "        img = img.convert('L')\n",
    "    img = img.resize(resize, Image.NEAREST)\n",
    "    return np.array(img)"
   ],
   "id": "1ade1e0455661b71",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T05:02:42.157737732Z",
     "start_time": "2025-12-30T05:02:42.126554676Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class RandomMapResetWrapper(gym.Wrapper):\n",
    "    def __init__(self, env, size=8, p=0.8):\n",
    "        super().__init__(env)\n",
    "        self.size = size\n",
    "        self.p = p\n",
    "        self.current_desc = None  # <-- new field\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        new_desc = generate_random_map(size=self.size, p=self.p)\n",
    "        self.current_desc = tuple(new_desc)  # hashable\n",
    "        self.env = gym.make(\n",
    "            \"FrozenLake-v1\",\n",
    "            desc=new_desc,\n",
    "            is_slippery=self.env.spec.kwargs.get(\"is_slippery\", True),\n",
    "            render_mode=self.env.render_mode\n",
    "        )\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "    def step(self, action):\n",
    "        return self.env.step(action)"
   ],
   "id": "d7a3f7daf2995675",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T05:02:42.306673256Z",
     "start_time": "2025-12-30T05:02:42.293612896Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class FrozenLakeCNN(nn.Module):\n",
    "    def __init__(self, action_size, grayscale=True):\n",
    "        super().__init__()\n",
    "        self.grayscale = grayscale\n",
    "        channels = 1 if grayscale else 3\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(channels, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        # Calculate feature size: ((84-8)//4 + 1) = 20 → ((20-4)//2 +1)=9 → (9-3+1)=7\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64 * 7 * 7, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, action_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (B, H, W, C) → (B, C, H, W)\n",
    "        x = x.permute(0, 3, 1, 2).float() / 255.0\n",
    "        return self.fc(self.conv(x))"
   ],
   "id": "b84c4e625082fd8",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T05:02:42.435594912Z",
     "start_time": "2025-12-30T05:02:42.419416515Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = map(np.stack, zip(*batch))\n",
    "        return state, action, reward, next_state, done\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ],
   "id": "dd9fa52b9cfbdd50",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T05:02:42.621853256Z",
     "start_time": "2025-12-30T05:02:42.606881977Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, modelClass, state_size, action_size, lr=1e-3, gamma=0.99,\n",
    "                 buffer_size=10000, batch_size=64, target_update=100):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.target_update = target_update\n",
    "\n",
    "        # Q-network and target network\n",
    "        self.q_net = modelClass(state_size, action_size).to(device)\n",
    "        self.target_net = modelClass(state_size, action_size).to(device)\n",
    "        self.optimizer = optim.Adam(self.q_net.parameters(), lr=lr)\n",
    "\n",
    "        # Sync target network\n",
    "        self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "\n",
    "        self.memory = ReplayBuffer(buffer_size)\n",
    "        self.step_count = 0\n",
    "\n",
    "    def act(self, state, epsilon=0.0):\n",
    "        if random.random() < epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.q_net(state)\n",
    "        return q_values.argmax().item()\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.push(state, action, reward, next_state, done)\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        # sample batch\n",
    "        states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)\n",
    "        states = torch.FloatTensor(states).to(device)\n",
    "        actions = torch.LongTensor(actions).to(device)\n",
    "        rewards = torch.FloatTensor(rewards).to(device)\n",
    "        next_states = torch.FloatTensor(next_states).to(device)\n",
    "        dones = torch.BoolTensor(dones).to(device)\n",
    "\n",
    "        # current q-values\n",
    "        current_q_values = self.q_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        # next q-values frm target network\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_net(next_states).max(1)[0]\n",
    "            target_q_values = rewards + (self.gamma * next_q_values * (~dones))\n",
    "\n",
    "        # compute loss\n",
    "        loss = nn.MSELoss()(current_q_values, target_q_values)\n",
    "\n",
    "        # Optimize\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # update target network periodically\n",
    "        self.step_count += 1\n",
    "        if self.step_count % self.target_update == 0:\n",
    "            self.target_net.load_state_dict(self.q_net.state_dict())\n"
   ],
   "id": "76ebb4fd5483c182",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T05:02:42.812839080Z",
     "start_time": "2025-12-30T05:02:42.799153498Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_env(render_mode=None, use_image=False, img_size=(84, 84), grayscale=True, random_map_every_reset=True):\n",
    "    # Start with a dummy env to get action space, but we’ll replace it on first reset\n",
    "    env = gym.make(\"FrozenLake8x8-v1\", is_slippery=False, render_mode=render_mode)\n",
    "    if random_map_every_reset:\n",
    "        env = RandomMapResetWrapper(env, size=8)\n",
    "    if use_image:\n",
    "        env = FrozenLakeImageWrapper(env, img_size=img_size, grayscale=grayscale)\n",
    "        state_shape = (*img_size, 1 if grayscale else 3)\n",
    "    else:\n",
    "        # For non-image mode, you’d also need to handle dynamic state space,\n",
    "        # but since you're using image mode, we focus on that.\n",
    "        state_shape = env.observation_space.n\n",
    "    action_size = env.action_space.n\n",
    "    return env, state_shape, action_size\n",
    "\n",
    "\n",
    "def train_dqn(agent, env, state_shape, action_size,\n",
    "              episodes=2000,\n",
    "              max_steps=100,\n",
    "              epsilon_start=1.0,\n",
    "              epsilon_end=0.01,\n",
    "              epsilon_decay=0.995, ):\n",
    "    global history_rewards, history_epsilons\n",
    "\n",
    "    scores = deque(maxlen=100)\n",
    "    recent_maps = deque(maxlen=100)\n",
    "\n",
    "    epsilon = epsilon_start\n",
    "    print(\"Starting training...\")\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        # state is now image - no one-hot needed\n",
    "        total_reward = 0\n",
    "\n",
    "        current_map = getattr(env.env, 'current_desc', None)\n",
    "        recent_maps.append(current_map)\n",
    "\n",
    "        for t in range(max_steps):\n",
    "            action = agent.act(state, epsilon)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # FIXED REWARD SHAPING (preserve original reward)\n",
    "            original_reward = reward\n",
    "            if not truncated and terminated:\n",
    "                if original_reward == 1.0:  # GOAL\n",
    "                    shaped_reward = 1.0\n",
    "                else:  # HOLE\n",
    "                    shaped_reward = -1.0\n",
    "            else:\n",
    "                shaped_reward = original_reward - 0.01  # Gentle penalty\n",
    "\n",
    "            agent.remember(state, action, shaped_reward, next_state, done)\n",
    "            agent.replay()\n",
    "\n",
    "            state = next_state\n",
    "            total_reward += shaped_reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        scores.append(total_reward)\n",
    "        epsilon = max(epsilon_end, epsilon_decay * epsilon)\n",
    "        history_rewards.append(np.mean(scores))\n",
    "        history_epsilons.append(epsilon)\n",
    "\n",
    "        if episode % 100 == 0:\n",
    "            unique_maps = len(set(m for m in recent_maps if m is not None))\n",
    "            print(f\"Episode {episode}, Avg Reward: {np.mean(scores):.3f}, \"\n",
    "                  f\"Epsilon: {epsilon:.3f}, Unique Maps (last 100): {unique_maps}\")\n",
    "\n",
    "        if episode % 1500 == 0 and episode > 0:\n",
    "            render_env, _, _ = create_env(render_mode=\"human\", use_image=True, random_map_every_reset=True)\n",
    "            evaluate_agent(agent, render_env, episodes=2, no_log=True)\n",
    "            render_env.close()\n",
    "\n",
    "    return agent, env\n",
    "\n",
    "\n",
    "def evaluate_agent(agent, env, episodes=10, max_steps=1000, no_log=False):\n",
    "    \"\"\"\n",
    "    Evaluate agent on pixel-based environment.\n",
    "    - Works with both image and discrete state spaces\n",
    "    \"\"\"\n",
    "    success = 0\n",
    "    for _ in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        for step in range(max_steps):\n",
    "            action = agent.act(state, epsilon=0.0)  # Greedy policy\n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "            if terminated or truncated:\n",
    "                if reward == 1.0:  # Reached goal\n",
    "                    success += 1\n",
    "                break\n",
    "\n",
    "    if not no_log:\n",
    "        print(f\"\\nSuccess rate: {success}/{episodes} ({100 * success / episodes:.1f}%)\")\n",
    "    return success\n",
    "\n",
    "\n",
    "def act(self, state, epsilon=0.0):\n",
    "    if random.random() < epsilon:\n",
    "        return random.randrange(self.action_size)\n",
    "    state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        q_values = self.q_net(state_tensor)\n",
    "    return q_values.argmax().item()"
   ],
   "id": "be994fe179396459",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## training loop",
   "id": "d101a4f785d545ba"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T05:02:43.626473947Z",
     "start_time": "2025-12-30T05:02:43.599703832Z"
    }
   },
   "cell_type": "code",
   "source": [
    "grayscale = True  # Set to False for RGB\n",
    "img_size = (84, 84)\n",
    "\n",
    "_, _, action_size = create_env(use_image=False)\n",
    "env, state_shape, _ = create_env(render_mode=\"rgb_array\", use_image=True,\n",
    "                                 img_size=img_size, grayscale=grayscale)\n",
    "\n",
    "agent = DQNAgent(\n",
    "    modelClass=lambda s, a: FrozenLakeCNN(a, grayscale=grayscale),\n",
    "    state_size=None,  # Not used\n",
    "    action_size=action_size,\n",
    "    lr=5e-5,\n",
    "    gamma=0.999,\n",
    "    buffer_size=50_000,\n",
    "    batch_size=64,\n",
    "    target_update=10\n",
    ")\n",
    "\n",
    "history_rewards, history_epsilons = [], []"
   ],
   "id": "e39de1e7d2e53f6e",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-12-30T05:02:44.173382715Z"
    }
   },
   "cell_type": "code",
   "source": [
    "try:\n",
    "    agent, env = train_dqn(\n",
    "        agent=agent,\n",
    "        env=env,\n",
    "        state_shape=state_shape,\n",
    "        action_size=action_size,\n",
    "        episodes=20_000,\n",
    "        max_steps=1000,\n",
    "        epsilon_start=1.0,\n",
    "        epsilon_end=0.01,\n",
    "        epsilon_decay=0.9995,\n",
    "    )\n",
    "finally:\n",
    "    # Plotting code (unchanged)\n",
    "    print(f\"Model: {FrozenLakeCNN.__name__}\")\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "    color = 'tab:blue'\n",
    "    ax1.set_xlabel('Episode')\n",
    "    ax1.set_ylabel('Avg Reward (last 100)', color=color)\n",
    "    ax1.plot(history_rewards, color=color, label='Avg Reward')\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    if len(history_rewards) > 0:\n",
    "        y_min = np.floor(min(history_rewards) * 10) / 10\n",
    "        y_max = np.ceil(max(history_rewards) * 10) / 10\n",
    "        ax1.set_yticks(np.arange(y_min, y_max + 0.05, 0.1))\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    color = 'tab:red'\n",
    "    ax2.set_ylabel('Epsilon', color=color)\n",
    "    ax2.plot(history_epsilons, color=color, label='Epsilon', linestyle='--')\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    plt.title('Training Progress')\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "4bf226c5f9d0f342",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Episode 0, Avg Reward: -1.130, Epsilon: 1.000, Unique Maps (last 100): 1\n",
      "Episode 100, Avg Reward: -1.131, Epsilon: 0.951, Unique Maps (last 100): 100\n",
      "Episode 200, Avg Reward: -1.131, Epsilon: 0.904, Unique Maps (last 100): 100\n",
      "Episode 300, Avg Reward: -1.131, Epsilon: 0.860, Unique Maps (last 100): 100\n",
      "Episode 400, Avg Reward: -1.113, Epsilon: 0.818, Unique Maps (last 100): 100\n",
      "Episode 500, Avg Reward: -1.106, Epsilon: 0.778, Unique Maps (last 100): 100\n",
      "Episode 600, Avg Reward: -1.107, Epsilon: 0.740, Unique Maps (last 100): 100\n",
      "Episode 700, Avg Reward: -1.066, Epsilon: 0.704, Unique Maps (last 100): 100\n",
      "Episode 800, Avg Reward: -1.106, Epsilon: 0.670, Unique Maps (last 100): 100\n",
      "Episode 900, Avg Reward: -1.068, Epsilon: 0.637, Unique Maps (last 100): 100\n",
      "Episode 1000, Avg Reward: -1.114, Epsilon: 0.606, Unique Maps (last 100): 100\n",
      "Episode 1100, Avg Reward: -1.091, Epsilon: 0.577, Unique Maps (last 100): 100\n",
      "Episode 1200, Avg Reward: -1.038, Epsilon: 0.548, Unique Maps (last 100): 100\n",
      "Episode 1300, Avg Reward: -1.097, Epsilon: 0.522, Unique Maps (last 100): 100\n",
      "Episode 1400, Avg Reward: -1.060, Epsilon: 0.496, Unique Maps (last 100): 100\n",
      "Episode 1500, Avg Reward: -1.038, Epsilon: 0.472, Unique Maps (last 100): 100\n",
      "Episode 1600, Avg Reward: -1.045, Epsilon: 0.449, Unique Maps (last 100): 100\n",
      "Episode 1700, Avg Reward: -1.118, Epsilon: 0.427, Unique Maps (last 100): 100\n",
      "Episode 1800, Avg Reward: -1.127, Epsilon: 0.406, Unique Maps (last 100): 100\n",
      "Episode 1900, Avg Reward: -1.098, Epsilon: 0.386, Unique Maps (last 100): 100\n",
      "Episode 2000, Avg Reward: -1.170, Epsilon: 0.368, Unique Maps (last 100): 100\n",
      "Episode 2100, Avg Reward: -1.160, Epsilon: 0.350, Unique Maps (last 100): 100\n",
      "Episode 2200, Avg Reward: -1.218, Epsilon: 0.333, Unique Maps (last 100): 100\n",
      "Episode 2300, Avg Reward: -1.248, Epsilon: 0.316, Unique Maps (last 100): 100\n",
      "Episode 2400, Avg Reward: -1.228, Epsilon: 0.301, Unique Maps (last 100): 100\n",
      "Episode 2500, Avg Reward: -1.218, Epsilon: 0.286, Unique Maps (last 100): 100\n",
      "Episode 2600, Avg Reward: -1.223, Epsilon: 0.272, Unique Maps (last 100): 100\n",
      "Episode 2700, Avg Reward: -1.234, Epsilon: 0.259, Unique Maps (last 100): 100\n",
      "Episode 2800, Avg Reward: -1.177, Epsilon: 0.246, Unique Maps (last 100): 100\n",
      "Episode 2900, Avg Reward: -1.186, Epsilon: 0.234, Unique Maps (last 100): 100\n",
      "Episode 3000, Avg Reward: -1.192, Epsilon: 0.223, Unique Maps (last 100): 100\n",
      "Episode 3100, Avg Reward: -1.136, Epsilon: 0.212, Unique Maps (last 100): 100\n",
      "Episode 3200, Avg Reward: -1.200, Epsilon: 0.202, Unique Maps (last 100): 100\n",
      "Episode 3300, Avg Reward: -1.165, Epsilon: 0.192, Unique Maps (last 100): 100\n",
      "Episode 3400, Avg Reward: -1.175, Epsilon: 0.183, Unique Maps (last 100): 100\n",
      "Episode 3500, Avg Reward: -1.203, Epsilon: 0.174, Unique Maps (last 100): 100\n",
      "Episode 3600, Avg Reward: -1.197, Epsilon: 0.165, Unique Maps (last 100): 100\n",
      "Episode 3700, Avg Reward: -1.230, Epsilon: 0.157, Unique Maps (last 100): 100\n",
      "Episode 3800, Avg Reward: -1.172, Epsilon: 0.149, Unique Maps (last 100): 100\n",
      "Episode 3900, Avg Reward: -1.222, Epsilon: 0.142, Unique Maps (last 100): 100\n",
      "Episode 4000, Avg Reward: -1.206, Epsilon: 0.135, Unique Maps (last 100): 100\n",
      "Episode 4100, Avg Reward: -1.185, Epsilon: 0.129, Unique Maps (last 100): 100\n",
      "Episode 4200, Avg Reward: -1.177, Epsilon: 0.122, Unique Maps (last 100): 100\n",
      "Episode 4300, Avg Reward: -1.238, Epsilon: 0.116, Unique Maps (last 100): 100\n",
      "Episode 4400, Avg Reward: -1.199, Epsilon: 0.111, Unique Maps (last 100): 100\n",
      "Episode 4500, Avg Reward: -1.185, Epsilon: 0.105, Unique Maps (last 100): 100\n",
      "Episode 4600, Avg Reward: -1.211, Epsilon: 0.100, Unique Maps (last 100): 100\n",
      "Episode 4700, Avg Reward: -1.193, Epsilon: 0.095, Unique Maps (last 100): 100\n",
      "Episode 4800, Avg Reward: -1.212, Epsilon: 0.091, Unique Maps (last 100): 100\n",
      "Episode 4900, Avg Reward: -1.207, Epsilon: 0.086, Unique Maps (last 100): 100\n",
      "Episode 5000, Avg Reward: -1.190, Epsilon: 0.082, Unique Maps (last 100): 100\n",
      "Episode 5100, Avg Reward: -1.191, Epsilon: 0.078, Unique Maps (last 100): 100\n",
      "Episode 5200, Avg Reward: -1.180, Epsilon: 0.074, Unique Maps (last 100): 100\n",
      "Episode 5300, Avg Reward: -1.170, Epsilon: 0.071, Unique Maps (last 100): 100\n",
      "Episode 5400, Avg Reward: -1.196, Epsilon: 0.067, Unique Maps (last 100): 100\n",
      "Episode 5500, Avg Reward: -1.168, Epsilon: 0.064, Unique Maps (last 100): 100\n",
      "Episode 5600, Avg Reward: -1.153, Epsilon: 0.061, Unique Maps (last 100): 100\n",
      "Episode 5700, Avg Reward: -1.131, Epsilon: 0.058, Unique Maps (last 100): 100\n",
      "Episode 5800, Avg Reward: -1.165, Epsilon: 0.055, Unique Maps (last 100): 100\n",
      "Episode 5900, Avg Reward: -1.151, Epsilon: 0.052, Unique Maps (last 100): 100\n",
      "Episode 6000, Avg Reward: -1.154, Epsilon: 0.050, Unique Maps (last 100): 100\n",
      "Episode 6100, Avg Reward: -1.145, Epsilon: 0.047, Unique Maps (last 100): 100\n",
      "Episode 6200, Avg Reward: -1.142, Epsilon: 0.045, Unique Maps (last 100): 100\n",
      "Episode 6300, Avg Reward: -1.182, Epsilon: 0.043, Unique Maps (last 100): 100\n",
      "Episode 6400, Avg Reward: -1.134, Epsilon: 0.041, Unique Maps (last 100): 100\n",
      "Episode 6500, Avg Reward: -1.171, Epsilon: 0.039, Unique Maps (last 100): 100\n",
      "Episode 6600, Avg Reward: -1.108, Epsilon: 0.037, Unique Maps (last 100): 100\n",
      "Episode 6700, Avg Reward: -1.178, Epsilon: 0.035, Unique Maps (last 100): 100\n",
      "Episode 6800, Avg Reward: -1.150, Epsilon: 0.033, Unique Maps (last 100): 100\n",
      "Episode 6900, Avg Reward: -1.112, Epsilon: 0.032, Unique Maps (last 100): 100\n",
      "Episode 7000, Avg Reward: -1.151, Epsilon: 0.030, Unique Maps (last 100): 100\n",
      "Episode 7100, Avg Reward: -1.146, Epsilon: 0.029, Unique Maps (last 100): 100\n",
      "Episode 7200, Avg Reward: -1.123, Epsilon: 0.027, Unique Maps (last 100): 100\n",
      "Episode 7300, Avg Reward: -1.155, Epsilon: 0.026, Unique Maps (last 100): 100\n",
      "Episode 7400, Avg Reward: -1.078, Epsilon: 0.025, Unique Maps (last 100): 100\n",
      "Episode 7500, Avg Reward: -1.065, Epsilon: 0.023, Unique Maps (last 100): 100\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "eval_env, _, _ = create_env(render_mode=\"human\", use_image=True, random_map_every_reset=True)\n",
    "evaluate_agent(agent, eval_env, episodes=5)\n",
    "eval_env.close()"
   ],
   "id": "cc67dba095c72fa2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
